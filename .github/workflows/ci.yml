name: CI

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]

env:
  CARGO_TERM_COLOR: always
  RUSTFLAGS: "-D warnings"

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy
      - uses: Swatinem/rust-cache@v2
      - run: cargo fmt --all -- --check
      - run: cargo clippy --all-targets --all-features -- -D warnings

  test:
    name: Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - run: cargo test --all-features --verbose
      - run: cargo test --no-default-features

  build-matrix:
    name: Build (${{ matrix.os }} / ${{ matrix.rust }})
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        rust: [stable, beta]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@master
        with:
          toolchain: ${{ matrix.rust }}
      - uses: Swatinem/rust-cache@v2
      - run: cargo build --all-targets --all-features
      - run: cargo test --all-features

  reference:
    name: Reference Comparison
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - name: Validate LIBSVM reference lock
        run: bash scripts/check_libsvm_reference_lock.sh
      - name: Build CLI tools
        run: cargo build --release -p svm-train-rs -p svm-predict-rs
      - name: Run reference comparisons
        run: bash scripts/compare_references.sh
      - name: Upload comparison report
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: reference-diff
          path: reference/diff_report.txt

  msrv:
    name: MSRV (1.75.0)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@1.75.0
      - run: cargo check --all-features

  security:
    name: Security audit
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - name: Install cargo-audit
        run: cargo install cargo-audit --locked
      - name: Run cargo-audit
        run: cargo audit

  coverage:
    name: Code coverage
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
        with:
          key: coverage
      - name: Install cargo-llvm-cov
        run: cargo install cargo-llvm-cov --locked
      - name: Enforce coverage thresholds
        run: bash scripts/check_coverage_thresholds.sh
      - name: Generate coverage
        run: cargo llvm-cov --workspace --all-features --lcov --output-path lcov.info
      - name: Upload to Codecov
        uses: codecov/codecov-action@v5
        with:
          files: lcov.info
          fail_ci_if_error: false

  bench:
    name: Benchmark
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - name: Run Rust vs C benchmark matrix
        run: python3 scripts/benchmark_compare.py
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report
          path: |
            reference/benchmark_report.md
            reference/benchmark_results.json

  wasm-integration:
    name: WASM Integration Benchmark
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
      - uses: actions/setup-node@v4
        with:
          node-version: "22"
      - uses: astral-sh/setup-uv@v5
      - name: Install Python plotting dependencies
        run: |
          uv venv .venv
          . .venv/bin/activate
          uv pip install numpy matplotlib
          echo "VIRTUAL_ENV=$PWD/.venv" >> "$GITHUB_ENV"
          echo "$PWD/.venv/bin" >> "$GITHUB_PATH"
      - name: Run wasm vs C++ benchmark smoke
        run: WASM_WARMUP=1 WASM_RUNS=6 bash examples/integrations/wasm_inference/run.sh
      - name: Validate wasm benchmark outputs
        run: |
          python3 - <<'PY'
          import json
          from pathlib import Path

          out = Path("examples/integrations/wasm_inference/output")
          results_file = out / "results.json"
          report_file = out / "report.md"
          local_plot = out / "wasm_vs_cpp.png"
          global_plot = Path("examples/comparison.png")
          global_summary = Path("examples/comparison_summary.json")

          assert results_file.exists(), f"missing {results_file}"
          assert report_file.exists(), f"missing {report_file}"
          assert local_plot.exists(), f"missing {local_plot}"
          assert global_plot.exists(), f"missing {global_plot}"
          assert global_summary.exists(), f"missing {global_summary}"

          payload = json.loads(results_file.read_text(encoding="utf-8"))
          rows = payload.get("results", [])
          assert len(rows) == 1, f"expected 1 result row, got {len(rows)}"
          row = rows[0]
          for op in ("train", "predict"):
            rust_samples = row["timing"][op]["rust"]["samples_ms"]
            c_samples = row["timing"][op]["c"]["samples_ms"]
            assert len(rust_samples) == 6, f"{op} rust sample count mismatch: {len(rust_samples)}"
            assert len(c_samples) == 6, f"{op} c sample count mismatch: {len(c_samples)}"
          assert row["metrics"]["prediction_agreement"] >= 0.99, "prediction agreement too low"

          summary = json.loads(global_summary.read_text(encoding="utf-8"))
          sources = summary.get("sources", [])
          assert any(
            "examples/integrations/wasm_inference/output/results.json" in s
            for s in sources
          ), "wasm integration results not included in global summary"
          print("WASM integration benchmark outputs validated.")
          PY
      - name: Upload wasm benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: wasm-integration-benchmark
          path: |
            examples/integrations/wasm_inference/output/results.json
            examples/integrations/wasm_inference/output/report.md
            examples/integrations/wasm_inference/output/wasm_vs_cpp.png
            examples/comparison.png
            examples/comparison_summary.json
